# PongRL

This project implements a Deep Q-Network (DQN) agent that learns to play the classic game of Pong.
The development history of this repository is synthetically generated to simulate a realistic project timeline over approximately 8 weeks.

## Core Features
-   **Pong Game Environment**: Built with Pygame, offering a Gym-like API (`reset`, `step`) for easy integration with RL agents. Includes customizable game parameters.
-   **DQN Agent**: Implemented in PyTorch, featuring:
    -   **Q-Network**: A configurable Multi-Layer Perceptron (MLP) to approximate action-value functions.
    -   **Experience Replay**: A replay buffer to store and sample past experiences, improving learning stability and efficiency.
    -   **Target Network**: A separate, periodically updated target Q-network to stabilize Q-learning updates.
    -   **Epsilon-Greedy Exploration**: A strategy for balancing exploration of new actions and exploitation of known good actions, with decaying epsilon.
-   **Model Persistence**: Functionality to save trained model weights and load them for continued training or evaluation.
-   **Training Script (`train.py`)**:
    -   Manages the agent-environment interaction loop.
    -   Logs training progress (scores, epsilon).
    -   Saves model checkpoints and the final trained model.
    -   Plots training scores using Matplotlib.
    -   Configurable via command-line arguments (e.g., number of episodes, save paths).
-   **Evaluation Script (`play.py`)**:
    -   Loads a pre-trained agent model.
    -   Renders the agent playing Pong in real-time.
    -   Configurable via command-line arguments (e.g., model path, number of games).

## Project Structure

The repository is organized as follows:

```
PongRL/
├── .gitignore                 # Specifies intentionally untracked files
├── README.md                  # This file
├── requirements.txt           # Python package dependencies
├── config.py                  # Global configurations: screen settings, RL hyperparameters
├── main.py                    # Script for human vs. simple AI play (uses PongEnv)
├── game/                      # Pong game environment modules
│   ├── __init__.py
│   ├── pong_env.py            # Core Pong game environment logic (Gym-like API)
│   ├── paddle.py              # Paddle class definition
│   └── ball.py                # Ball class definition
├── agent/                     # DQN agent modules
│   ├── __init__.py
│   ├── dqn_agent.py           # Main DQNAgent class (handles learning, acting, etc.)
│   ├── model.py               # PyTorch Q-Network model architecture (nn.Module)
│   └── replay_buffer.py       # Experience Replay Buffer implementation
├── utils/                     # Utility modules (currently a placeholder)
│   ├── __init__.py
│   └── helpers.py             # Placeholder for helper functions
├── train.py                   # Script to train the DQN agent
├── play.py                    # Script to watch a trained agent play
├── models/                    # Directory for saved model weights (e.g., dqn_pong_final.pth)
│   └── .gitkeep               # Ensures the directory is tracked even if empty initially
├── notes/                     # Directory for development notes, logs, or research
│   └── .gitkeep
└── training_scores.png        # Example plot generated by train.py after training
```

## System Requirements
-   Python 3.7+
-   Pygame (`pip install pygame`)
-   NumPy (`pip install numpy`)
-   PyTorch (`pip install torch torchvision torchaudio`) - CPU or GPU version.
-   Matplotlib (`pip install matplotlib`) - For plotting training scores.

It is recommended to install these dependencies using the provided `requirements.txt` file, preferably within a virtual environment:

```bash
# Create and activate a virtual environment (optional but recommended)
# python -m venv venv
# source venv/bin/activate  # On Linux/macOS
# .\venv\Scripts\activate    # On Windows

pip install -r requirements.txt
```

## How to Use

### 1. Training the Agent
To train the DQN agent from scratch, run the `train.py` script:
```bash
python train.py --episodes 3000 --save_path models/my_pong_model.pth --plot_path my_training_plot.png
```
-   The agent will be trained for the specified number of episodes (points scored).
-   The trained model weights will be saved to the path provided by `--save_path`.
-   A plot illustrating the training progress (scores per episode) will be saved to the path provided by `--plot_path`.
-   Checkpoints may also be saved periodically during training.

Run `python train.py --help` to see all available command-line options.

### 2. Watching a Trained Agent
Once an agent model is trained and saved (e.g., to `models/my_pong_model.pth`), you can watch it play:
```bash
python play.py --model models/my_pong_model.pth --games 5 --points 3 --delay 0.03
```
-   This command will load the specified model.
-   The agent will play 5 full games.
-   A game is won when either player reaches 3 points.
-   A delay of 0.03 seconds is introduced between frames for better visual observation.

Run `python play.py --help` for all available options.

### 3. Human vs. Simple AI (Legacy/Demo)
The `main.py` script provides a basic version of Pong where a human can play against a simple, rule-based AI opponent using the W/S keys for paddle movement. This utilizes the `PongEnv`.
```bash
python main.py
```
